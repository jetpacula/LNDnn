{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ucs08zzxJgUg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.color import rgba2rgb\n",
    "from getpass import getpass\n",
    "import boto3 #using creds \n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "#bucket = \"arn:aws:s3:::lnd-bucket-aa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip  LNDnn.ipynb  Longitudinal_Nutrient_Deficiency  __MACOSX\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3 # этот кусок для скачки датасета без аккаунта\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "with open('data.zip', 'wb') as f:\n",
    "    s3.download_fileobj('intelinair-data-releases', 'longitudinal-nutrient-deficiency/Longitudinal_Nutrient_Deficiency.zip', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_key_id = getpass(prompt='your account key: ', stream=None) #нужно для подгрузки модели на S3\n",
    "my_secret_key = getpass(prompt='your account secret key: ', stream=None)\n",
    "s3 = boto3.client('s3',\n",
    "    aws_access_key_id=my_key_id,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "with open('Longitudinal_Nutrient_Deficiency', 'wb') as f:\n",
    "    s3.download_fileobj(, 'longitudinal-nutrient-deficiency/Longitudinal_Nutrient_Deficiency.zip', f)\n",
    "with open('filename', 'wb') as data:\n",
    "    s3.download_fileobj('mybucket', 'mykey', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "with open('data.zip', 'wb') as f:\n",
    "    s3.download_fileobj('intelinair-data-releases', 'longitudinal-nutrient-deficiency/Longitudinal_Nutrient_Deficiency.zip', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq ./data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "bucket = \"lnd-bucket-aa\"\n",
    "\n",
    "folder = 'Longitudinal_Nutrient_Deficiency/'\n",
    "\n",
    "s3uri = 's3://{}/{}'.format(bucket, folder)\n",
    "\n",
    "#df = pd.read_csv(s3uri)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    " When running on SageMaker, need execution role\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "#Declare bucket name, remote file, and destination\n",
    "my_bucket = 'lnd-bucket-aa'\n",
    "orig_file = 'Longitudinal_Nutrient_Deficiency/'\n",
    "#dest_file = 'local/path/to/file.p'\n",
    "\n",
    " Connect to S3 bucket and download file\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(my_bucket).download_file(orig_file, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_RfHjvrNCi1",
    "outputId": "3a078454-0aa4-4593-a274-230d7fa4cc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bfk8o2imLY98",
    "outputId": "cb318451-fa05-4ea9-b32a-b1eb1b569447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "\n",
    "#tb = SummaryWriter(comment='myboard')\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "device.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlmfGMJL-7h_"
   },
   "source": [
    "## Готовим датасет\n",
    "\n",
    "Скачиваем данные, передаем трансформер (предобработку), визуализируем  примеры семплов, создаем dataloaders для обучения и тестирования модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "s1BPUh3WIZr0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "     #   self.img_labels = os.walk(self.img_dir)][1:]\n",
    "        self.img_labels = [x[0]+\"/nutrient_mask_g0.png\" for x in os.walk(datasetDir)][1:]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     #   lst = [x[0] for x in os.walk(datasetDir)][1:][]\n",
    "      #  folder_list = [x[0] for x in os.walk(self.img_dir)][1:]\n",
    "#pil_image = PIL.Image.open('Image.jpg').convert('RGB') \n",
    "\n",
    "        image = PIL.Image.open(os.path.join(datasetDir, \"field_\"+\"{:03d}\".format(idx+1)+\"/image_i0.png\")).convert('RGB') #убираем ф\n",
    "        label =  PIL.Image.open(os.path.join(datasetDir, \"field_\"+\"{:03d}\".format(idx+1)+\"/nutrient_mask_g0.png\")).convert('1')\n",
    "   #     image = rgba2rgb(PIL.Image.open(os.path.join(datasetDir, \"field_\"+\"{:03d}\".format(idx+1)+\"/image_i0.png\")))\n",
    "    #    label =  rgba2rgb(PIL.Image.open(os.path.join(datasetDir, \"field_\"+\"{:03d}\".format(idx+1)+\"/nutrient_mask_g0.png\")))\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)#[:,:,:3]\n",
    "            label = self.transform(label)#[:,:,:3]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tiRn0fHSDEc9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "datasetDir = os.path.join(os.getcwd(),'Longitudinal_Nutrient_Deficiency')\n",
    "mydataset = CustomDataset(img_dir = datasetDir, transform = transforms.Compose([transforms.Resize(256)\n",
    "    , transforms.CenterCrop(256),transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rZvNZEXcsOAx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, testing_data = train_test_split(mydataset, test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zU5w1N2WA2pv"
   },
   "outputs": [],
   "source": [
    "train_loader =  torch.utils.data.DataLoader(training_data, \n",
    "                                          batch_size=120, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(testing_data, \n",
    "                                          batch_size=120, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GgQ36a9DHh2q"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=20):\n",
    "    start = time.time()\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            step = 0            \n",
    "            for x, y in dataloader:\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                step += 1\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "                acc = acc_fn(outputs, y)\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    \n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cpu()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def numpy_iou(outs,labels, threshold=0.5):# IoU calculation\n",
    "    outs = (outs > threshold)\n",
    "    intersection = numpy.logical_and(outs.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "    union = numpy.logical_or(outs.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "    iou_score = numpy.sum(intersection) / numpy.sum(union)\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\n",
      "  Downloading segmentation_models_pytorch-0.2.1-py3-none-any.whl (88 kB)\n",
      "     |████████████████████████████████| 88 kB 5.3 MB/s             \n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
      "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 1.5 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "     |████████████████████████████████| 376 kB 28.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from segmentation_models_pytorch) (0.8.2)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.7.1)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.62.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (8.4.0)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (4.0.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (0.8)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=96c51f4763d859d5035539a6176824bf4f477eddd131043ef4af224a756079a0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1c/07/d1/ff77968842daae1dde944173a8e8a7be193646d37842f13b24\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=deef6bf2bcbb429df40eda3fb2a90249a69fe597c26874ca6af267ee4bde1e35\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/9c/03/81/494596f1d017bbf80daf53bc33dbb05aaa5a6ad6d44c74011f\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "loss_fn = smp.losses.TverskyLoss(\"binary\", alpha=2, beta=5, gamma=5)\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"timm-efficientnet-b1\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "5DipVlxDHjHB",
    "outputId": "5c803648-29de-4d12-fdd2-0941b342e95e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c82bc056b7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumpy_iou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#torch.save(model.state_dict(), \"tversky_loss_SMOL_5_epochs_iou_10fields_DeepLab_NOTquantized.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9533bfea6317>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mrunning_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "train_loss, valid_loss = train(model,train_loader,test_loader,loss_fn,opt,numpy_iou,epochs=2)\n",
    "#torch.save(model.state_dict(), \"tversky_loss_SMOL_5_epochs_iou_10fields_DeepLab_NOTquantized.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jetpacula/miniforge3/envs/firstEnv/lib/python3.9/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9269 Acc: 0.06267140060663223\n",
      "valid Loss: 0.9036 Acc: 0.06853586435317993\n",
      "Training complete in 10m 32s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cb/md7wxr255j7906jg2_h6c3bc0000gn/T/ipykernel_5723/784331230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou_pytorch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dice_5epochs_iou.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "unet = UNET(3,1)\n",
    "loss_fn = DiceLoss()\n",
    "opt = torch.optim.Adam(unet.parameters(),lr=0.01)\n",
    "train_loss, valid_loss = train(unet,train_loader,test_loader,loss_fn,opt,iou_pytorch,epochs=20)\n",
    "torch.save(unet.state_dict(), \"Dice_20epochs_iou.h5\")\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, '')).upload_file('Dice_20epochs_iou.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c0c94592ca5ad0d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c0c94592ca5ad0d5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb.close()\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_fn = FocalLoss\n",
    "opt = torch.optim.Adam(unet.parameters(),lr=0.01)\n",
    "train_loss, valid_loss = train(unet,train_loader,test_loader,loss_fn,opt,iou_pytorch,epochs=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "myPractice1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
